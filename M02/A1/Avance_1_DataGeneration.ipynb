{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115f18b7",
   "metadata": {},
   "source": [
    "# Fleetlogix - Proyecto Integrador Data Science\n",
    "\n",
    "Fleetlogix es una empresa de transporte y logística que opera una flota de 200 vehículos realizando entregas de última milla en cinco ciudades principales de Colombia.  \n",
    "Este proyecto integra modelado relacional, generación de datos sintéticos masivos, validaciones de calidad, KPIs logísticos y una arquitectura cloud escalable.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos\n",
    "- Poblar una base de datos PostgreSQL con más de **500.000 registros sintéticos** generados con Python.\n",
    "- Garantizar integridad referencial y reglas de negocio mediante claves primarias, foráneas e índices.\n",
    "- Documentar el modelo relacional con un **diagrama ERD** y un **modelo dimensional tipo estrella** para OLAP.\n",
    "- Implementar queries SQL para validaciones de calidad y KPIs operativos.\n",
    "- Proponer una arquitectura cloud con Kafka, Flink/Spark, Data Lake y Power BI para análisis en tiempo real.\n",
    "\n",
    "---\n",
    "\n",
    "## Modelo Relacional (ERD)\n",
    "El modelo relacional se compone de seis tablas:\n",
    "\n",
    "- **Maestras:** `vehicles`, `drivers`, `routes`  \n",
    "- **Transaccionales:** `trips`, `deliveries`, `maintenance`\n",
    "\n",
    "Restricciones técnicas:\n",
    "- Claves primarias y foráneas para integridad referencial.\n",
    "- Restricciones de unicidad en placas, licencias y tracking numbers.\n",
    "- Índices en campos críticos para optimizar queries.\n",
    "\n",
    "![Diagrama ERD](assets/Diagrama_Fleetlogix.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Generación de Datos Sintéticos\n",
    "**Herramientas:** Python + Faker + pandas + numpy + psycopg2  \n",
    "\n",
    "**Volumen generado:**\n",
    "- 200 vehículos  \n",
    "- 400 conductores  \n",
    "- 50 rutas  \n",
    "- 100.000 viajes  \n",
    "- 400.001 entregas  \n",
    "- 5.000 mantenimientos  \n",
    "\n",
    "> Nota: Se generaron 400.001 entregas en lugar de 400.000 debido a la naturaleza probabilística del generador.  \n",
    "> Cada viaje recibe entre 2 y 6 entregas según distribución aleatoria controlada, lo que produce pequeñas variaciones alrededor del valor esperado.\n",
    "\n",
    "---\n",
    "\n",
    "## Validaciones de Calidad\n",
    "- Integridad referencial: todas las claves foráneas válidas.  \n",
    "- Consistencia temporal: `arrival_datetime > departure_datetime`.  \n",
    "- Reglas de negocio:  \n",
    "  - Peso ≤ capacidad del vehículo  \n",
    "  - Tracking numbers únicos  \n",
    "  - Entregas por viaje entre 2 y 6  \n",
    "\n",
    "Ejemplo de validación:\n",
    "```sql\n",
    "-- Trips sin vehículo válido (resultado esperado: 0)\n",
    "SELECT COUNT(*) AS invalid_trips\n",
    "FROM trips t\n",
    "LEFT JOIN vehicles v ON v.vehicle_id = t.vehicle_id\n",
    "WHERE v.vehicle_id IS NULL;\n",
    "```\n",
    "## KPIs Operativos\n",
    "- Porcentaje de entregas a tiempo vs retrasadas\n",
    "- Consumo promedio de combustible por tipo de vehículo\n",
    "- Utilización de capacidad por viaje\n",
    "- Mantenimientos por cada 1.000 km\n",
    "- Promedio de entregas por viaje\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "```sql\n",
    "-- Entregas a tiempo vs con retraso\n",
    "WITH delivered AS (\n",
    "    SELECT delivery_id,\n",
    "           CASE WHEN delivered_datetime <= scheduled_datetime + INTERVAL '30 minutes'\n",
    "                THEN 'on_time' ELSE 'late' END AS status\n",
    "    FROM deliveries\n",
    ")\n",
    "SELECT status, COUNT(*) AS cnt,\n",
    "       ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS pct\n",
    "FROM delivered\n",
    "WHERE status IS NOT NULL\n",
    "GROUP BY status;\n",
    "```\n",
    "Resultado: 90.15% entregas a tiempo, 9.85% con retraso.\n",
    "\n",
    "## Propuesta de Arquitectura en la Nube\n",
    "- Ingesta: PostgreSQL + CDC (Debezium)\n",
    "- Streaming: Apache Kafka\n",
    "- Procesamiento: Apache Flink / Spark Streaming\n",
    "- Almacenamiento: Data Lake + Data Warehouse\n",
    "- Visualización: Power BI / Looker\n",
    "- Gobernanza: Data Catalog + Lineage\n",
    "\n",
    "La arquitectura cloud funciona como un flujo continuo que integra captura, transmisión, procesamiento, almacenamiento y visualización de datos. Los cambios en PostgreSQL se detectan en tiempo real mediante CDC y se envían a través de Kafka. Flink o Spark Streaming procesan los eventos de manera paralela, los resultados se guardan en un Data Lake y en un Data Warehouse, y finalmente dashboards interactivos permiten decisiones rápidas con gobernanza que asegura calidad y trazabilidad.\n",
    "\n",
    "\n",
    "## Ejecución\n",
    "1. Abrir PowerShell y ubicarse en la carpeta de scripts:\n",
    "\n",
    "```powershell\n",
    "cd \"RUTA\\LOCAL\\DE\\LA\\CARPETA\"\n",
    ".\\run_fleetlogix.ps1\n",
    "```\n",
    "2. Crear conexión en DBeaver:\n",
    "    - Host: localhost\n",
    "    - Port: 5432\n",
    "    - Database: fleetlogixdb\n",
    "    - Username: postgres\n",
    "    - Password: definida en cada script de acuerdo a cada configuración local (o al servidor)\n",
    "\n",
    "3. Ejecutar queries de validación y KPIs:\n",
    "    - 01_InventarioDeTablasPKsFKsIndices.sql\n",
    "    - 02_ValidacionesCalidadConsistencia.sql\n",
    "    - 03_KPIsOperativosConsultasClave.sql\n",
    "\n",
    "\n",
    "## Elaborado por Federico Ceballos Torres\n",
    "### Avance 1 del Módulo 2\n",
    "### Proyecto integrador\n",
    "### Carrera Data Science\n",
    "## SoyHenry"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
